{
  "args": {
    "common": {
      "num_layers": 80,
      "hidden_size": 8192,
      "ffn_hidden_size": 24576,
      "max_sequence_length": 32768,
      "num_attention_heads": 64,
      "max_position_embeddings": 32768,
      "vocab_size": 152064
    },
    "huggingface": {
      "architectures": [
        "QWenLMHeadModel"
      ],
      "auto_map": {
        "AutoConfig": "configuration_qwen.QWenConfig",
        "AutoModelForCausalLM": "modeling_qwen.QWenLMHeadModel"
      },
      "attn_dropout_prob": 0.0,
      "bf16": false,
      "emb_dropout_prob": 0.0,
      "fp16": false,
      "fp32": false,
      "hidden_size": 8192,
      "intermediate_size": 49152,
      "initializer_range": 0.02,
      "kv_channels": 128,
      "layer_norm_epsilon": 1e-06,
      "max_position_embeddings": 32768,
      "model_type": "qwen",
      "no_bias": true,
      "num_attention_heads": 64,
      "num_hidden_layers": 80,
      "onnx_safe": null,
      "rope_theta": 1000000,
      "rotary_emb_base": 1000000,
      "rotary_pct": 1.0,
      "scale_attn_weights": true,
      "seq_length": 32768,
      "tie_word_embeddings": false,
      "tokenizer_class": "QWenTokenizer",
      "use_cache": true,
      "use_dynamic_ntk": false,
      "use_flash_attn": "auto",
      "use_logn_attn": false
    },
    "megatron": {
      "untie_embeddings_and_output_weights": true,
      "num_layers_per_virtual_pipeline_stage": null,
      "virtual_pipeline_model_parallel_size": null,
      "use_rotary_position_embeddings": true,
      "add_embedding_padding": true,
      "make_vocab_size_divisible_by": 128,
      "transpose_mlp_dense": true,
      "transpose_query_key_value": true
    },
    "mcore": {
      "untie_embeddings_and_output_weights": true,
      "num_layers_per_virtual_pipeline_stage": null,
      "virtual_pipeline_model_parallel_size": null,
      "use_rotary_position_embeddings": true,
      "add_embedding_padding": true,
      "make_vocab_size_divisible_by": 128,
      "transpose_mlp_dense": true,
      "transpose_query_key_value": true
    }
  },
  "name_map": {
    "huggingface": {
      "word_embeddings": "transformer.wte",
      "transformer": "transformer",
      "layer_prefix": "h",
      "input_layernorm": "ln_1",
      "attention.query_key_value": "attn.c_attn",
      "attention.dense": "attn.c_proj",
      "post_attention_layernorm": "ln_2",
      "mlp.dense_h_to_4h": [
        "mlp.w2",
        "mlp.w1"
      ],
      "mlp.dense_4h_to_h": "mlp.c_proj",
      "final_layernorm": "transformer.ln_f",
      "word_embeddings_for_head": "lm_head"
    },
    "megatron": {
      "word_embeddings": "model.language_model.embedding.word_embeddings",
      "word_position_embeddings": "model.language_model.embedding.position_embeddings",
      "transformer": "model.language_model.encoder",
      "layer_prefix": "layers",
      "input_layernorm": "input_layernorm",
      "attention.query_key_value": "self_attention.query_key_value",
      "attention.dense": "self_attention.dense",
      "post_attention_layernorm": "post_attention_layernorm",
      "mlp.dense_h_to_4h": "mlp.dense_h_to_4h",
      "mlp.dense_4h_to_h": "mlp.dense_4h_to_h",
      "final_layernorm": "final_layernorm",
      "word_embeddings_for_head": "model.language_model.output_layer",
      "word_embeddings_tpl": "model%d.language_model.embedding.word_embeddings",
      "word_position_embeddings_tpl": "model%d.language_model.embedding.position_embeddings",
      "transformer_tpl": "model%d.language_model.encoder",
      "word_embeddings_for_head_tpl": "model%d.language_model.output_layer"
    },
    "mcore": {
      "word_embeddings": "embedding.word_embeddings",
      "word_position_embeddings": "model.embedding.position_embeddings",
      "transformer": "model",
      "layer_prefix": "decoder.layers",
      "input_layernorm": "self_attention.linear_qkv.layer_norm",
      "attention.query_key_value": "self_attention.linear_qkv",
      "attention.dense": "self_attention.linear_proj",
      "post_attention_layernorm": "mlp.linear_fc1.layer_norm",
      "mlp.dense_h_to_4h": "mlp.linear_fc1",
      "mlp.dense_4h_to_h": "mlp.linear_fc2",
      "final_layernorm": "decoder.final_layernorm",
      "word_embeddings_for_head": "output_layer",
      "transformer_tpl": "model%d"
    }
  },
  "tensor_parallel_dim": {
    "word_embeddings.weight": 0,
    "attention.query_key_value.weight": 0,
    "attention.query_key_value.bias": 0,
    "attention.dense.weight": 1,
    "mlp.dense_h_to_4h.weight": 0,
    "mlp.dense_h_to_4h.bias": 0,
    "mlp.dense_4h_to_h.weight": 1,
    "word_embeddings_for_head.weight": 0
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.32.0"
}