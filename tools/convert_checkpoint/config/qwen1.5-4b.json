{
  "args": {
    "common": {
      "num_layers": 40,
      "hidden_size": 2560,
      "ffn_hidden_size": 6912,
      "num_attention_heads": 20,
      "max_position_embeddings": 32768,
      "vocab_size": 151936
    },
    "huggingface": {
      "architectures": [
        "Qwen2ForCausalLM"
      ],
      "attention_dropout": 0.0,
      "bos_token_id": 151643,
      "eos_token_id": 151643,
      "hidden_act": "silu",
      "hidden_size": 2560,
      "initializer_range": 0.02,
      "intermediate_size": 6912,
      "max_position_embeddings": 32768,
      "max_window_layers": 21,
      "model_type": "qwen2",
      "num_attention_heads": 20,
      "num_hidden_layers": 40,
      "num_key_value_heads": 20,
      "rms_norm_eps": 1e-06,
      "rope_theta": 5000000.0,
      "sliding_window": 32768,
      "tie_word_embeddings": false,
      "torch_dtype": "bfloat16",
      "use_cache": true,
      "use_sliding_window": false
    },
    "mcore": {
      "untie_embeddings_and_output_weights": true,
      "num_layers_per_virtual_pipeline_stage": null,
      "virtual_pipeline_model_parallel_size": null,
      "use_rotary_position_embeddings": true,
      "add_embedding_padding": true,
      "make_vocab_size_divisible_by": 128,
      "transpose_mlp_dense": true,
      "transpose_query_key_value": true
    }
  },
  "name_map": {
    "huggingface": {
      "word_embeddings": "model.embed_tokens",
      "transformer": "model",
      "layer_prefix": "layers",
      "input_layernorm": "input_layernorm",
      "attention.query_key_value": [
        "self_attn.q_proj",
        "self_attn.k_proj",
        "self_attn.v_proj"
      ],
      "attention.dense": "self_attn.o_proj",
      "post_attention_layernorm": "post_attention_layernorm",
      "mlp.dense_h_to_4h": [
        "mlp.gate_proj",
        "mlp.up_proj"
      ],
      "mlp.dense_4h_to_h": "mlp.down_proj",
      "final_layernorm": "model.norm",
      "word_embeddings_for_head": "lm_head"
    },
    "mcore": {
      "word_embeddings": "embedding.word_embeddings",
      "word_position_embeddings": "model.embedding.position_embeddings",
      "transformer": "model",
      "layer_prefix": "decoder.layers",
      "input_layernorm": "self_attention.linear_qkv.layer_norm",
      "attention.query_key_value": "self_attention.linear_qkv",
      "attention.dense": "self_attention.linear_proj",
      "post_attention_layernorm": "mlp.linear_fc1.layer_norm",
      "mlp.dense_h_to_4h": "mlp.linear_fc1",
      "mlp.dense_4h_to_h": "mlp.linear_fc2",
      "final_layernorm": "decoder.final_layernorm",
      "word_embeddings_for_head": "output_layer",
      "transformer_tpl": "model%d"
    }
  },
  "tensor_parallel_dim": {
    "word_embeddings.weight": 0,
    "attention.query_key_value.weight": 0,
    "attention.query_key_value.bias": 0,
    "attention.dense.weight": 1,
    "mlp.dense_h_to_4h.weight": 0,
    "mlp.dense_h_to_4h.bias": 0,
    "mlp.dense_4h_to_h.weight": 1,
    "word_embeddings_for_head.weight": 0
  },
  "torch_dtype": "bfloat16",
  "transformers_version": "4.37.0"
}